<head>
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>
    <meta charset="utf-8">
</head>

<style>
    body.me{
        text-align: justify;
    }
</style>

<p style="text-align: left">[<a href="https://markgriswold.substack.com/p/thoughts-on-over-a-decade-of-magnetic">Previous</a>] <span style="float: right;">[Next]</span></p>
<hr>
<h2>Deep image prior</h2>
<p>Jun-06-2024</p>
<hr>
<body class="me">
    <p>Basic information realted to "Deep Image Prior (DIP)":
        <ul>
        <li><b>Reference:</b> Dmitry Ulyanov et al. <a href="https://arxiv.org/abs/1711.10925">Deep Image Prior</a>. arXiv:1711.10925 (2017)</li> </p>
        <li><b>Sources:</b> [<a href="https://github.com/DmitryUlyanov/deep-image-prior">Code</a>] [<a href="https://www.youtube.com/watch?v=-g1NsTuP1_I">Youtube</a>] [<a href="https://dmitryulyanov.github.io/deep_image_prior">Project Page</a>]</li>
    </ul>
    <h3>1. What's the idea of deep image prior (DIP)?</h3>
    <p><b>[Method Section of DIP paper]</b></p>
    <p>A deep generator network is a parametric function \(x = f_{\theta}(z)\) that map a code vector \(z\) to an image \(x\). Generators are often used to model a complex distribution \(p(x)\) over images as teh transformation of simple distribution \(p(z)\) over the codes, such as Gaussian distribution.</p>
    <p>Some people may think that the distribution \(p(x)\) is encoded in the parameters \(\theta\) of the network model. However, the authors of DIP hold the idea that a significant amount of information about the image distribution is contained in the <i>structure</i> of the network even without performing any training of model parameters. </p>
    <p>The generalized equation for tasks such as denosing, impainting, and superresolution etc.: 
    $${x^* = \underset{x}{\mathrm {argmin}} E(x;x_0) + R(x) }$$
    where \(x_0 \) is the noisy/occluded/low resolution image, and \(E(x; x_0)\) is a task-dependent data term. \(R(x)\) is a regularizer, such as TV/nuclear norm/ wavelet transformation. </p>
    <p>Instead, the DIP will replace \(R(x)\) with implicit prior captured by the neural network parametrization:
        $${ \theta^* = \underset{\theta}{\mathrm{argmain}} E(f_{\theta}(z); x_0)}, \;\;\;\; {x^* = f_{\theta^*}(z)}$$
    where the (local) minimizer \(\theta^*\) is obtained using an optimizer such as gradient descent, starting from a <i>random initialization</i> of the parameters \(\theta\). 
    </p>

    <h3>2. DIP in MRI</h3>
    <p>I found several articles using DIP in dymamic imaging, motion correction, and parameter mapping: <ul>
        <li>Jaejun Yoo. et al. <a href="https://pubmed.ncbi.nlm.nih.gov/34043506/">Time-Dependent Deep Image Prior for Dynamic MRI</a>. IEEE TMI (2021) [<a href="https://github.com/jaejun-yoo/TDDIP">Code</a>]</li>
        <li>Jongyeon Lee. et al <a href="https://onlinelibrary.wiley.com/doi/full/10.1002/mrm.30026">Unsupervised motion artifact correction of turbo spin-echo MRI using deep image prior</a>. MRM (2024)</li>
        <li>Hamilton JI. et al. <a href="https://pubmed.ncbi.nlm.nih.gov/38098428/">Deep image prior cine MR fingerprinting with \(B_1^+\) spin history correction</a>. MRM (2024)</li>
    </ul></p>


    <hr>
</body>
<p style="text-align: center;"> &copy; 2024 wrr6ps | All rights reserved </p>