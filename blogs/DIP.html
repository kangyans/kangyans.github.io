<head>
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>
    <meta charset="utf-8">
</head>

<style>
    body.me{
        text-align: justify;
    }

    .nav{
      line-height:1.2;
      text-align:end;
    }

</style>

  <div class="nav">
      [<a href="..\index.html">Home</a>] [<a href="..\Projects.html">Projects</a>] [<a href="..\Publications.html">Publications</a>] [<a href="..\Blogs.html">Blogs</a>] [<a href="..\Misc.html">Misc</a>] [<a href="..\Misc.html">Misc</a>] [<a href="..\faq.html">FAQ</a>]
  </div>
<h2>Deep image prior</h2>
<p>Jun-06-2024</p>
<hr>
<body class="me">
    <p>Basic information realted to "Deep Image Prior (DIP)":
        <ul>
        <li><p><b>Reference:</b> Dmitry Ulyanov et al. <a href="https://arxiv.org/abs/1711.10925">Deep Image Prior</a>. arXiv:1711.10925 (2017)</li>
        <li><p><b>Sources:</b> [<a href="https://github.com/DmitryUlyanov/deep-image-prior">Code</a>] [<a href="https://www.youtube.com/watch?v=-g1NsTuP1_I">Youtube</a>] [<a href="https://dmitryulyanov.github.io/deep_image_prior">Project Page</a>]</p></li>
    </ul>
    </p>
    <h3>1. What's the idea of deep image prior (DIP)?</h3>
    <p><b>[Method Section of DIP paper]</b></p>
    <p>A deep generator network is a parametric function \(x = f_{\theta}(z)\) that map a code vector \(z\) to an image \(x\). Generators are often used to model a complex distribution \(p(x)\) over images as the transformation of simple distribution \(p(z)\) over the codes, such as Gaussian distribution.</p>
    <p>Some people may think that the distribution \(p(x)\) is encoded in the parameters \(\theta\) of the network model. However, the authors of DIP hold the idea that a significant amount of information about the image distribution is contained in the <i>structure</i> of the network even without performing any training of model parameters. </p>
    <p>The generalized equation for tasks such as denosing, impainting, and superresolution etc.: 
    $${x^* = \underset{x}{\mathrm {argmin}} E(x;x_0) + R(x) }$$
    where \(x_0 \) is the noisy/occluded/low resolution image, and \(E(x; x_0)\) is a task-dependent data term. \(R(x)\) is a regularizer, such as TV/nuclear norm/ wavelet transformation. </p>
    <p>Instead, the DIP will replace \(R(x)\) with implicit prior captured by the neural network parametrization:
        $${ \theta^* = \underset{\theta}{\mathrm{argmain}} E(f_{\theta}(z); x_0)}, \;\;\;\; {x^* = f_{\theta^*}(z)}$$
    where the (local) minimizer \(\theta^*\) is obtained using an optimizer such as gradient descent, starting from a <i>random initialization</i> of the parameters \(\theta\). 
    </p>

    <h3>2. DIP in MRI</h3>
    <p>I found several articles using DIP in dynamic imaging, motion correction, and parameter mapping, their links are given as follows: <ul>
        <li><p>Jaejun Yoo. et al. <a href="https://pubmed.ncbi.nlm.nih.gov/34043506/">Time-Dependent Deep Image Prior for Dynamic MRI</a>. IEEE TMI (2021) [<a href="https://github.com/jaejun-yoo/TDDIP">Code</a>]</p></li>
        <li><p>Zhongsen Li. et al. <a href="https://arxiv.org/abs/2403.15770">Graph Image Prior for Unsupervised Dynamic Cardiac Cine MRI Reconstruction</a>. arXiv (2024)</p></li>
        <li><p>Jongyeon Lee. et al <a href="https://onlinelibrary.wiley.com/doi/full/10.1002/mrm.30026">Unsupervised motion artifact correction of turbo spin-echo MRI using deep image prior</a>. MRM (2024)</p></li>
        <li><p>Hamilton JI. et al. <a href="https://pubmed.ncbi.nlm.nih.gov/38098428/">Deep image prior cine MR fingerprinting with \(B_1^+\) spin history correction</a>. MRM (2024)</p></li>
    </ul></p>
    <p>In dynamic imaging, instead of reconstructing each single image at each time point independently, the temporal dependencies of dynamic measurements should be taken into consideration. In TDDIP method, they used a one-dimensional manifold parametrized by time to learn the time-dependencies. Even though a temporally meaningful manifolds was chosen, the hand-crafted design will limit the performance. Therefore, the author introduced a MapNet [<a href="https://openaccess.thecvf.com/content_CVPR_2019/html/Karras_A_Style-Based_Generator_Architecture_for_Generative_Adversarial_Networks_CVPR_2019_paper.html">1</a>] to add flexibility to their model. The MapNet involves some fully connected layers with nonlinearities, which is for mapping a fixed manifold into the more expressive latent space. \(\Omega = g_{\phi}(Z)\)</p>
    <p>Zhongsen Li et al., however, think that the TDDIP model would limit the model expression because it employed a pyramid-shaped CNN generator shared by all image frames. Therefore they proposed a two stage generative network: (1). employing independent CNN to recover the image of each time point; (2) exploiting the spatio-temporal correlations within the feature space parameterized by a graph model(GNN). </p> 
    <h3>3. Q&A</h3>
    <h4>1. Don't need to train MapNet(TDDIP)/GNN (Graph image prior) either?</h4>
    <p> You should not only take the CNN network \(h_\psi\) as the parameterized network \(f_\theta\). Instead, the whole network \(f_\theta = g_\phi \circ h_\psi\)</p>

    <span style="color:red;">[to be countined]</span>

</body>
<hr>
<p style="text-align: center;"> &copy; 2024 wrr6ps</p>
